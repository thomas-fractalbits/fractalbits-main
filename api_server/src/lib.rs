mod blob_storage;
mod config;
pub mod handler;
mod object_layout;

use blob_storage::{
    BlobStorage, BlobStorageError, BlobStorageImpl, BssOnlySingleAzStorage, HybridSingleAzStorage,
    S3ExpressMultiAzStorage, S3ExpressMultiAzWithTracking, S3ExpressSingleAzStorage,
    S3ExpressWithTrackingConfig,
};
use bucket_tables::{table::KvClientProvider, Versioned};
use bytes::Bytes;
pub use config::{BlobStorageBackend, BlobStorageConfig, Config, S3HybridConfig};
pub use data_blob_tracking::{DataBlobTracker, DataBlobTrackingError};
use futures::stream::{self, StreamExt};
use metrics::histogram;
use moka::future::Cache;
use rpc_client_nss::RpcClientNss;
use rpc_client_rss::{RpcClientRss, RpcErrorRss};

use slotmap_conn_pool::{ConnPool, Poolable};
use std::{
    sync::Arc,
    time::{Duration, Instant},
};
use tokio::{
    sync::mpsc::{self, Receiver, Sender},
    task::JoinHandle,
};
use tracing::info;
use uuid::Uuid;

pub type BlobId = uuid::Uuid;

pub struct AppState {
    pub config: Arc<Config>,
    pub cache: Arc<Cache<String, Versioned<String>>>,
    pub az_status_cache: Option<Arc<Cache<String, String>>>,

    rpc_clients_nss: ConnPool<Arc<RpcClientNss>, String>,
    rpc_clients_rss: ConnPool<Arc<RpcClientRss>, String>,

    blob_client: Arc<BlobClient>,
    blob_deletion: Sender<(String, BlobId, usize)>,
    pub data_blob_tracker: Arc<DataBlobTracker>,
}

impl KvClientProvider for AppState {
    type Error = RpcErrorRss;

    async fn checkout_rpc_client_rss(
        &self,
    ) -> Result<
        impl kv_client_traits::KvClient<Error = Self::Error>,
        <RpcClientRss as Poolable>::Error,
    > {
        let start = Instant::now();
        let res = self
            .rpc_clients_rss
            .checkout(self.config.rss_addr.clone())
            .await?;
        histogram!("checkout_rpc_client_nanos", "type" => "rss")
            .record(start.elapsed().as_nanos() as f64);
        Ok(res)
    }
}

impl AppState {
    pub async fn new(config: Arc<Config>) -> Self {
        let rpc_clients_rss =
            Self::new_rpc_clients_pool_rss(&config.rss_addr, config.rss_conn_num).await;
        let rpc_clients_nss =
            Self::new_rpc_clients_pool_nss(&config.nss_addr, config.nss_conn_num).await;

        let (tx, rx) = mpsc::channel(1024 * 1024);
        let data_blob_tracker = Arc::new(DataBlobTracker::with_pools(
            config.rss_addr.clone(),
            rpc_clients_rss.clone(),
            config.nss_addr.clone(),
            rpc_clients_nss.clone(),
        ));

        let cache = Arc::new(
            Cache::builder()
                .time_to_idle(Duration::from_secs(300))
                .max_capacity(10_000)
                .build(),
        );

        let (blob_client, az_status_cache) = BlobClient::new(
            &config.blob_storage,
            rx,
            config.rpc_timeout(),
            Some(data_blob_tracker.clone()),
        )
        .await
        .expect("Failed to initialize blob client");

        Self {
            config,
            rpc_clients_nss,
            blob_client: Arc::new(blob_client),
            blob_deletion: tx,
            rpc_clients_rss,
            cache,
            az_status_cache,
            data_blob_tracker,
        }
    }

    async fn new_rpc_clients_pool_nss(
        nss_addr: &str,
        nss_conn_num: u16,
    ) -> ConnPool<Arc<RpcClientNss>, String> {
        let rpc_clients_nss = ConnPool::new();

        // Use the Poolable trait's retry logic instead of manual retry
        for i in 0..nss_conn_num as usize {
            info!(
                "Connecting to NSS server at {nss_addr} (connection {}/{})",
                i + 1,
                nss_conn_num
            );
            let client = Arc::new(
                <RpcClientNss as slotmap_conn_pool::Poolable>::new(nss_addr.to_string())
                    .await
                    .unwrap(),
            );
            rpc_clients_nss.pooled(nss_addr.to_string(), client);
        }

        info!("NSS RPC client pool initialized with {nss_conn_num} connections.");
        rpc_clients_nss
    }

    async fn new_rpc_clients_pool_rss(
        rss_addr: &str,
        rss_conn_num: u16,
    ) -> ConnPool<Arc<RpcClientRss>, String> {
        let rpc_clients_rss = ConnPool::new();

        // Use the Poolable trait's retry logic
        for i in 0..rss_conn_num as usize {
            info!(
                "Connecting to RSS server at {rss_addr} (connection {}/{})",
                i + 1,
                rss_conn_num
            );
            let client = Arc::new(
                <RpcClientRss as slotmap_conn_pool::Poolable>::new(rss_addr.to_string())
                    .await
                    .unwrap(),
            );
            rpc_clients_rss.pooled(rss_addr.to_string(), client);
        }

        info!("RSS RPC client pool initialized with {rss_conn_num} connections.");
        rpc_clients_rss
    }

    pub async fn checkout_rpc_client_nss(
        &self,
    ) -> Result<Arc<RpcClientNss>, <RpcClientNss as Poolable>::Error> {
        let start = Instant::now();
        let res = self
            .rpc_clients_nss
            .checkout(self.config.nss_addr.clone())
            .await?;
        histogram!("checkout_rpc_client_nanos", "type" => "nss")
            .record(start.elapsed().as_nanos() as f64);
        Ok(res)
    }

    pub async fn checkout_rpc_client_rss_direct(
        &self,
    ) -> Result<Arc<RpcClientRss>, <RpcClientRss as Poolable>::Error> {
        let start = Instant::now();
        let res = self
            .rpc_clients_rss
            .checkout(self.config.rss_addr.clone())
            .await?;
        histogram!("checkout_rpc_client_nanos", "type" => "rss")
            .record(start.elapsed().as_nanos() as f64);
        Ok(res)
    }

    pub fn get_blob_client(&self) -> Arc<BlobClient> {
        self.blob_client.clone()
    }

    pub fn get_blob_deletion(&self) -> Sender<(String, BlobId, usize)> {
        self.blob_deletion.clone()
    }
}

pub struct BlobClient {
    storage: Arc<BlobStorageImpl>,
    #[allow(dead_code)]
    blob_deletion_task_handle: JoinHandle<()>,
}

impl BlobClient {
    pub async fn new(
        blob_storage_config: &BlobStorageConfig,
        rx: Receiver<(String, BlobId, usize)>,
        rpc_timeout: Duration,
        data_blob_tracker: Option<Arc<DataBlobTracker>>,
    ) -> Result<(Self, Option<Arc<Cache<String, String>>>), BlobStorageError> {
        let storage = match &blob_storage_config.backend {
            BlobStorageBackend::BssOnlySingleAz => {
                let bss_config = blob_storage_config.bss.as_ref().ok_or_else(|| {
                    BlobStorageError::Config(
                        "BSS configuration required for BssOnly backend".into(),
                    )
                })?;
                BlobStorageImpl::BssOnlySingleAz(
                    BssOnlySingleAzStorage::new(&bss_config.addr, bss_config.conn_num, rpc_timeout)
                        .await,
                )
            }
            BlobStorageBackend::HybridSingleAz => {
                let bss_config = blob_storage_config.bss.as_ref().ok_or_else(|| {
                    BlobStorageError::Config("BSS configuration required for Hybrid backend".into())
                })?;
                let s3_cache_config = blob_storage_config
                    .s3_hybrid_single_az
                    .as_ref()
                    .ok_or_else(|| {
                        BlobStorageError::Config(
                            "S3 hybrid configuration required for Hybrid backend".into(),
                        )
                    })?;
                BlobStorageImpl::HybridSingleAz(
                    HybridSingleAzStorage::new(
                        &bss_config.addr,
                        bss_config.conn_num,
                        s3_cache_config,
                        rpc_timeout,
                    )
                    .await,
                )
            }
            BlobStorageBackend::S3ExpressMultiAz => {
                let s3_express_config = blob_storage_config
                    .s3_express_multi_az
                    .as_ref()
                    .ok_or_else(|| {
                        BlobStorageError::Config(
                            "S3 Express configuration required for S3Express backend".into(),
                        )
                    })?;
                let express_config = blob_storage::S3ExpressMultiAzConfig {
                    local_az_host: s3_express_config.local_az_host.clone(),
                    local_az_port: s3_express_config.local_az_port,
                    remote_az_host: s3_express_config.remote_az_host.clone(),
                    remote_az_port: s3_express_config.remote_az_port,
                    s3_region: s3_express_config.s3_region.clone(),
                    local_az_bucket: s3_express_config.local_az_bucket.clone(),
                    remote_az_bucket: s3_express_config.remote_az_bucket.clone(),
                    az: s3_express_config.local_az.clone(),
                    rate_limit_config: blob_storage::S3RateLimitConfig::from(
                        &s3_express_config.ratelimit,
                    ),
                    retry_config: s3_express_config.retry_config.clone(),
                };
                BlobStorageImpl::S3ExpressMultiAz(
                    S3ExpressMultiAzStorage::new(&express_config).await?,
                )
            }
            BlobStorageBackend::S3ExpressMultiAzWithTracking => {
                let s3_express_config = blob_storage_config
                    .s3_express_multi_az
                    .as_ref()
                    .ok_or_else(|| {
                        BlobStorageError::Config(
                            "S3 Express configuration required for S3ExpressWithTracking backend"
                                .into(),
                        )
                    })?;
                let data_blob_tracker = data_blob_tracker.ok_or_else(|| {
                    BlobStorageError::Config(
                        "DataBlobTracker required for S3ExpressWithTracking backend".into(),
                    )
                })?;
                let express_config = S3ExpressWithTrackingConfig {
                    local_az_host: s3_express_config.local_az_host.clone(),
                    local_az_port: s3_express_config.local_az_port,
                    s3_region: s3_express_config.s3_region.clone(),
                    local_az_bucket: s3_express_config.local_az_bucket.clone(),
                    remote_az_bucket: s3_express_config.remote_az_bucket.clone(),
                    remote_az_host: s3_express_config.remote_az_host.clone(),
                    remote_az_port: s3_express_config.remote_az_port,
                    local_az: s3_express_config.local_az.clone(),
                    remote_az: s3_express_config.remote_az.clone(),
                    rate_limit_config: blob_storage::S3RateLimitConfig::from(
                        &s3_express_config.ratelimit,
                    ),
                    retry_config: s3_express_config.retry_config.clone(),
                };

                // Create a separate cache for AZ status
                let az_status_cache = Arc::new(
                    Cache::builder()
                        .time_to_idle(std::time::Duration::from_secs(30)) // Shorter TTL for AZ status
                        .max_capacity(100) // Small cache size
                        .build(),
                );

                let storage = BlobStorageImpl::S3ExpressMultiAzWithTracking(
                    S3ExpressMultiAzWithTracking::new(
                        &express_config,
                        data_blob_tracker,
                        az_status_cache.clone(),
                    )
                    .await?,
                );

                let storage = Arc::new(storage);

                let blob_deletion_task_handle = tokio::spawn({
                    let storage = storage.clone();
                    async move {
                        if let Err(e) = Self::blob_deletion_task(storage, rx).await {
                            tracing::error!("FATAL: blob deletion task error: {e}");
                        }
                    }
                });

                return Ok((
                    Self {
                        storage,
                        blob_deletion_task_handle,
                    },
                    Some(az_status_cache),
                ));
            }
            BlobStorageBackend::S3ExpressSingleAz => {
                let s3_express_single_az_config = blob_storage_config
                    .s3_express_single_az
                    .as_ref()
                    .ok_or_else(|| {
                        BlobStorageError::Config(
                            "S3 Express Single AZ configuration required for S3ExpressSingleAz backend"
                                .into(),
                        )
                    })?;
                let single_az_config = blob_storage::S3ExpressSingleAzConfig {
                    s3_host: s3_express_single_az_config.s3_host.clone(),
                    s3_port: s3_express_single_az_config.s3_port,
                    s3_region: s3_express_single_az_config.s3_region.clone(),
                    s3_bucket: s3_express_single_az_config.s3_bucket.clone(),
                    az: s3_express_single_az_config.az.clone(),
                    force_path_style: s3_express_single_az_config.force_path_style,
                    rate_limit_config: blob_storage::S3RateLimitConfig::from(
                        &s3_express_single_az_config.ratelimit,
                    ),
                    retry_config: s3_express_single_az_config.retry_config.clone(),
                };
                BlobStorageImpl::S3ExpressSingleAz(
                    S3ExpressSingleAzStorage::new(&single_az_config).await?,
                )
            }
        };
        let storage = Arc::new(storage);

        let blob_deletion_task_handle = tokio::spawn({
            let storage = storage.clone();
            async move {
                if let Err(e) = Self::blob_deletion_task(storage, rx).await {
                    tracing::error!("FATAL: blob deletion task error: {e}");
                }
            }
        });

        Ok((
            Self {
                storage,
                blob_deletion_task_handle,
            },
            None,
        ))
    }

    async fn blob_deletion_task(
        storage: Arc<BlobStorageImpl>,
        mut input: Receiver<(String, BlobId, usize)>,
    ) -> Result<(), BlobStorageError> {
        while let Some((tracking_root_blob_name, blob_id, block_numbers)) = input.recv().await {
            let deleted = stream::iter(0..block_numbers)
                .map(|block_number| {
                    let storage = storage.clone();
                    let tracking_root = tracking_root_blob_name.clone();
                    async move {
                        let res = storage
                            .delete_blob(Some(&tracking_root), blob_id, block_number as u32)
                            .await;
                        match res {
                            Ok(()) => 1,
                            Err(e) => {
                                tracing::warn!("delete {blob_id}-p{block_number} failed: {e}");
                                0
                            }
                        }
                    }
                })
                .buffer_unordered(10)
                .fold(0, |acc, x| async move { acc + x })
                .await;
            let failed = block_numbers - deleted;
            if failed != 0 {
                tracing::warn!("delete parts of {blob_id}: ok={deleted},err={failed}");
            }
        }
        Ok(())
    }

    pub async fn put_blob(
        &self,
        tracking_root_blob_name: Option<&str>,
        blob_id: Uuid,
        block_number: u32,
        body: Bytes,
    ) -> Result<(), BlobStorageError> {
        self.storage
            .put_blob(tracking_root_blob_name, blob_id, block_number, body)
            .await
    }

    pub async fn get_blob(
        &self,
        blob_id: Uuid,
        block_number: u32,
        body: &mut Bytes,
    ) -> Result<(), BlobStorageError> {
        self.storage.get_blob(blob_id, block_number, body).await
    }

    pub async fn delete_blob(
        &self,
        tracking_root_blob_name: Option<&str>,
        blob_id: Uuid,
        block_number: u32,
    ) -> Result<(), BlobStorageError> {
        self.storage
            .delete_blob(tracking_root_blob_name, blob_id, block_number)
            .await
    }
}
